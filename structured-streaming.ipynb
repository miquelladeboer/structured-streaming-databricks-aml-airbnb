{"cells":[{"cell_type":"markdown","source":["## Lab: Building Structured Streaming Pipelines\n---\n### Pre-Lab Activity\n#### Refer to the Word Document \"Pre-Lab or Pre-Demo Instructions for Module 4\" to ensure everything is configured correctly\n---\n\n**What's Required for this Lab?**\n- Azure Event  Hubs Namespace\n- Azure Event Hub\n  - This refers to a hub being created on the namespace\n- Connection String for the created Azure Event Hub\n- Python installed in enviornment\n\n___\n\n### Pre-Lab Steps   \nIf you have completed the steps in the \"Pre-Lab or Pre-Demo Instructions for Module 4\" Word Document you can skip these steps\n___\n\n### Step 1: Install necessary libraries\n----\nFor this lab you will need more than the standard Python libraries on your local computer to run the sender.py. The Sender application makes use of `azure.servicebus`. Therefore, install the `azure-servicebus` library by running `pip install azure-servicebus==0.21.1`.\n  \n**NOTE**: Please use version 0.21.1 when installing azure-servicebus\n  \n### Step 2: Provision Azure Event Hubs\n---\n1. Go to the Azure Portal\n2. Click \"Create a Resource\"\n3. Look for Azure Event Hubs\n4. Create a Event Hub with the following specifications:  \n    - Name: {choose your own name}\n    - Pricing Tier: Basic\n    - Make this namespace zone redundant: Leave Disabled/Unchecked\n    - Subscription: Use auto-populated subscription, unless directed to use another by instructor\n    - Resource Group: {choose your own}\n    - Location: {choose whichever is most appropriate}\n    - Throughput Units: 1\n    - Enable Auto-Inflate: Leave Disabled/Unchecked\n5. Within the provisioned Azure Event Hubs Namespace, create a Event Hub\n6. Get the Connection String and Name of the Hub for the Event Hub  \n### Step 3: Edit the `sender.py` File\n---\nOpen `sender.py`, which can be found in the Lab Folder, in a code editor of your choice.  \n1. In line 8, where it says `sbs = ServiceBusService(service_namespace='<hub_name>', shared_access_key_name='<key_name>', shared_access_key_value='<key_value>')` put in the correct values for the name of the hub, the key name and the value of the key. All of these can be found in the Azure Portal in the Azure Event Hub that you provisioned in Step 2.\n2. In line 47, put your hub name in place of `<hub-name>`\n### Step 4: Run the `sender.py` program\n---\n1. Open Command Prompt, or some environment that allows you to run Python files\n2. Navigate to the directory where `sender.py` resides\n3. Run the following command `python sender.py`\n    - This will send messages to the Event Hub for a little more than 2 hours giving you enough time to complete this lab"],"metadata":{}},{"cell_type":"markdown","source":["### Lab Activities  \nFollow the set up guide \"Pre-Lab or Pre-Demo Instructions for Module 4\"\n\n---\n\n### Step 1: Establish connection between Databricks `readStream` event and Azure Event Hub\n---\nIn the following cell, create the configuration object and the `readStream` event that is necessary to create the streaming DataFrame for data coming into the Azure Event Hub.  \n  \n**Notes**:  \n- `format` option should be `org.apache.spark.sql.eventhubs.EventHubsSourceProvider`\n- Use the double asterisks in the `.option(**eventHubConfigurationObject)` in the `readStream` event"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.mount(\n  source = \"wasbs://productswithreference@productstoragestreaming.blob.core.windows.net\",\n  mount_point = \"/mnt/products\",\n  extra_configs = {\"fs.azure.account.key.productstoragestreaming.blob.core.windows.net\":dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"secretproduct\")})\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["connectionString = \"Endpoint=sb://demoeventhubtwitter.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=QVtgInlF0mNVrMXpQoddywlPVueBL6evPRmH1wRz+ZQ=;EntityPath=demotwittereh\"\n\n# Event Hubs Connection Configuration\nehConf = {\n  'eventhubs.connectionString' : connectionString\n}\n\nproductsSoldStream = spark \\\n  .readStream \\\n  .format(\"org.apache.spark.sql.eventhubs.EventHubsSourceProvider\") \\\n  .options(**ehConf) \\\n  .load()\ndisplay(productsSoldStream)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Step 2: Import necessary libarries for using `StructType`, `StructField`, and the `from_json` function\n---\nThe libraries are `pyspark.sql.types` and `pyspark.sql.functions`"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Step 3: Create the schema for the incoming data\n---\nTake a look at the data coming into the Azure Event Hub and create an appropriate schema. **Note**: Timestamp will not be a timestamp field, it must be a string."],"metadata":{}},{"cell_type":"code","source":["productsSchema = StructType([\n  StructField(\"room_id\", IntegerType(), True),\n  StructField(\"survey_id\", IntegerType(), True),\n  StructField(\"host_id\", IntegerType(), True),\n  StructField(\"room_type\", StringType(), True),\n  StructField(\"neighborhood\", StringType(), True),\n  StructField(\"reviews\", IntegerType(), True),\n  StructField(\"overall_satisfaction\", DoubleType(), True),\n  StructField(\"accommodates\", IntegerType(), True),\n  StructField(\"bedrooms\", IntegerType(), True),\n  StructField(\"price\", DoubleType(), True),\n  StructField(\"name\", StringType(), True),\n  StructField(\"last_modified\", StringType(), True),\n  StructField(\"latitude\", DoubleType(), True),\n  StructField(\"longitude\", DoubleType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Step 4: Grab the body and the enqueuedTime\n---\nSelect the body of the message as well as the enqueuedTime of the message from the streaming DataFrame created by the `readStream` event.  \n  \n**Notes**  \n- Remember how to select the body of the message from Event Hub, it needs to be `cast` into some data type\n- Think about giving an alias to enqueuedTime, possibly ArrivalTime?"],"metadata":{}},{"cell_type":"code","source":["productsRaw = productsSoldStream.select(productsSoldStream.body.cast('string'), productsSoldStream.enqueuedTime.alias('ArrivalTime'))\ndisplay(productsRaw)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Step 5: Use `from_json` to break down the JSON data\n---\nRemember to add the schema in with the `from_json` function, and remember to select the `enqueuedTime` along with the JSON data."],"metadata":{}},{"cell_type":"code","source":["productsJson = productsRaw.select(from_json(productsRaw.body, schema=productsSchema).alias(\"json\"), productsRaw.ArrivalTime)\ndisplay(productsJson)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Step 6: Breakdown the `struct` into individual columns\n---\nUse the `\"<col_name>.*\"` syntax to grab all the values in the `struct` object and put them into individual columns. Remember to also select the `enqueuedTime`."],"metadata":{}},{"cell_type":"code","source":["products = productsJson.select(\"json.*\", productsJson.ArrivalTime)\ndisplay(products)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Step 7: Create and Import `products` table\n---\nIn the directory with `sender.py` and this notebook you will find a small CSV file named `products.csv`. Drag this into your Databricks workspace and create a table.  \n  \nNow, once, imported, read the table into this notebook."],"metadata":{}},{"cell_type":"code","source":["productsInfo = spark.read.table(\"neigbourhoodinfo\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Step 8: Join the Streaming DataFrame and the Static DataFrame\n---\nUsing the streaming DataFrame created in **Step 6** and the static DataFrame created in **Step 11**, join them together on the `ProductType` column."],"metadata":{}},{"cell_type":"code","source":["products = products.join(productsInfo, on=\"neighborhood\")\ndisplay(products)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Step 9: Create the `writeStream` event\n---\nCreate a `writeStream` event that uses Databricks Delta. Additionally, add a trigger that will have the engine write every two processing seconds. Remember to add a location to store checkpoints. Have the event write to a table directly, and name that table `storeSalesDelta`.  \n  \n**Notes**  \n- The `format` option should be `delta`\n- The trigger is set using the `processingTime` value"],"metadata":{}},{"cell_type":"code","source":["products.writeStream \\\n          .format(\"delta\") \\\n          .trigger(processingTime = \"2 seconds\") \\\n          .option(\"checkpointLocation\", \"/mod4/lab1/sink/checkpoints/\") \\\n          .table(\"storeSalesDelta\")\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["checkpoint_path_silver='/mnt/products/checkpoints/silver/'\nsilver_path='/mnt/products/silver/'\nproducts.writeStream\\\n  .format(\"delta\") \\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_path_silver) \\\n  .start(silver_path)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["checkpoint_path_gold='/mnt/products/checkpoints/gold/'\ngold_path='/mnt/products/gold/'\nproducts.writeStream\\\n  .format(\"csv\") \\\n  .outputMode(\"append\")\\\n  .option(\"checkpointLocation\", checkpoint_path_gold) \\\n  .start(gold_path)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Step 10: Find the Average Profit for Every 5 Minute Interval\n---\nUsing SQL and the expression `AVG((price-CostOfProduction)) as AverageProfit` find the average profit grouped by a window of 5 minutes based on `enqueuedTime`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT AVG((price-CostOfProduction)) as AverageProfit\nFROM storeSalesDelta\nGROUP BY WINDOW(ArrivalTime, '5 minutes')"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Step 11: Run a `OPTIMIZE` and `VACUUM` job on the `storeSalesDelta` table\n---\nUsing SQL, run the `OPTIMIZE` and `VACUUM` jobs on the table crated in **Step 9**"],"metadata":{}},{"cell_type":"code","source":["%sql\nOPTIMIZE storeSalesDelta"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\nVACUUM storeSalesDelta"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"Module 4 - Lab 2: Building Structured Streaming Pipelines - Instructor","notebookId":3177111643931164},"nbformat":4,"nbformat_minor":0}
